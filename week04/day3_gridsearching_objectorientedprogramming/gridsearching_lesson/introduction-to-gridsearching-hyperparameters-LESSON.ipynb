{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Gridsearching Hyperparameters\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"gridsearch_meme.png\" style=\"width: 500px;\">\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand what the terms gridsearch and hyperparameter refer to.\n",
    "- Understand how to manually build a gridsearching procedure.\n",
    "- Apply sklearn's `GridSearchCV` object with basketball data to optimize a KNN model.\n",
    "- Practice using and evaluating attributes of the gridsearch object.\n",
    "- Understand the pitfalls of searching large hyperparameter spaces.\n",
    "- Practice the gridsearch procedure independently optimizing regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture: What part of the modelling process are we focusing on this morning?\n",
    "\n",
    "---\n",
    "\n",
    "As we looked at yesterday, one general approach you can use for modelling questions would look like this:\n",
    "\n",
    "- STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier.\n",
    "\n",
    "- STEP TWO: Set up predictor matrix (X) and target array (y).  **Dummify if necessary**.\n",
    "\n",
    "- STEP THREE: Train/test split and StandardScaler( )\n",
    "\n",
    "- STEP FOUR: Use cross-validation to **optimise the hyperparameters for your model**. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV).\n",
    "\n",
    "- STEP FIVE: Once you're happy with your hyperparameters, **fit your model on your whole training data** and test it on your whole testing data.\n",
    "\n",
    "- STEP SIX: Then you might want to **evaluate the performance** of the model (R2 score, accuracy, confusion matrix, etc); find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualise how well your model performed; investigate feature importance with .coef_ if you have a parametric model.\n",
    "\n",
    "This morning, we're learning more about STEP FOUR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## What is \"Gridsearching\"? What are \"hyperparameters\"?\n",
    "\n",
    "---\n",
    "\n",
    "Models often have **specifications that can be set**. For example, when we choose a linear regression, we may decide to **add a penalty to the loss function** such as the Ridge or the Lasso. Those penalties require the **regularization strength**, alpha, to be set. \n",
    "\n",
    "**Model parameters are called hyperparameters.**\n",
    "\n",
    "Hyperparameters are different than the parameters of the model resulting from a fit, such as the coefficients. The hyperparameters are **set prior to the fit** and determine the behavior of the model.\n",
    "\n",
    "There are often more than one kind of hyperparamter to set for a model. For example, in the KNN algorithm, \n",
    "- we have a hyperparameter to set the **number of neighbors**. \n",
    "- We also have a hyperparameter for the weights: **uniform or distance**?\n",
    "\n",
    "We want to know the *optimal* hyperparameter settings, the set that results in the best model evaluation. \n",
    "\n",
    "**The search for the optimal set of hyperparameters is called gridsearching.**\n",
    "\n",
    "Gridsearching gets its name from the fact that we are searching over a **\"grid\" of parameters**. For example, imagine the `n_neighbors` hyperparameters on the x-axis and `weights` on the y-axis, and we need to test all points on the grid.\n",
    "\n",
    "**Gridsearching uses cross-validation internally to evaluate the performance of each set of hyperparameters.** More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basketball-data'></a>\n",
    "\n",
    "## Basketball data\n",
    "\n",
    "---\n",
    "\n",
    "To explore the process of gridsearching over sets of hyperparameters, we will use some basketball data. The data below has statistics for 4 different seasons of NBA basketball: 2013-2016.\n",
    "- This data includes aggregate statistical data for each game. \n",
    "- The data of each game is aggregated by match for all players.\n",
    "- Scraped from http://www.basketball-refrence.com\n",
    "\n",
    "Many of the columns in the dataset represent the mean of a statistic across the last 10 games, for example. Non-target statistics are for *prior* games, they do not include information about player performance in the current game.\n",
    "\n",
    "**We are interested in predicting whether the home team will win the game or not.** This is a classification problem.\n",
    "\n",
    "\n",
    "### Load the data and create the target and predictor matrix\n",
    "- The target will be a binary column of whether the home team wins.\n",
    "- The predictors should be numeric statistics columns.\n",
    "\n",
    "Exclude these columns from the predictor matrix:\n",
    "\n",
    "    ['GameId','GameDate','GameTime','HostName',\n",
    "     'GuestName','total_score','total_line','game_line',\n",
    "     'winner','loser','host_wins','Season']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP ONE: Cleaning, descriptive stats, correlation heatmap, plots & visualisations. Find baseline accuracy if it's a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/basketball_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>GameId</th>\n",
       "      <th>GameDate</th>\n",
       "      <th>GameTime</th>\n",
       "      <th>HostName</th>\n",
       "      <th>GuestName</th>\n",
       "      <th>total_score</th>\n",
       "      <th>total_line</th>\n",
       "      <th>game_line</th>\n",
       "      <th>Host_HostRank</th>\n",
       "      <th>...</th>\n",
       "      <th>gPTS_avg10</th>\n",
       "      <th>gTS%_avg10</th>\n",
       "      <th>g3PAR_avg10</th>\n",
       "      <th>gFTr_avg10</th>\n",
       "      <th>gDRB%_avg10</th>\n",
       "      <th>gTRB%_avg10</th>\n",
       "      <th>gAST%_avg10</th>\n",
       "      <th>gSTL%_avg10</th>\n",
       "      <th>gBLK%_avg10</th>\n",
       "      <th>gDRtg_avg10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212090LAL</td>\n",
       "      <td>2012-12-09</td>\n",
       "      <td>6:30 pm</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>227.0</td>\n",
       "      <td>207.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2981</td>\n",
       "      <td>69.22</td>\n",
       "      <td>50.05</td>\n",
       "      <td>61.57</td>\n",
       "      <td>8.63</td>\n",
       "      <td>10.31</td>\n",
       "      <td>110.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100PHI</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>201.0</td>\n",
       "      <td>186.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>90.3</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>71.46</td>\n",
       "      <td>49.48</td>\n",
       "      <td>59.83</td>\n",
       "      <td>6.48</td>\n",
       "      <td>9.46</td>\n",
       "      <td>107.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212100HOU</td>\n",
       "      <td>2012-12-10</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>240.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>74.26</td>\n",
       "      <td>50.99</td>\n",
       "      <td>61.82</td>\n",
       "      <td>8.30</td>\n",
       "      <td>6.85</td>\n",
       "      <td>101.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110BRK</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:00 pm</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>197.0</td>\n",
       "      <td>195.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>100.3</td>\n",
       "      <td>0.5473</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>74.23</td>\n",
       "      <td>47.88</td>\n",
       "      <td>52.07</td>\n",
       "      <td>9.31</td>\n",
       "      <td>7.64</td>\n",
       "      <td>109.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>201212110DET</td>\n",
       "      <td>2012-12-11</td>\n",
       "      <td>7:30 pm</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>195.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>101.1</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>68.45</td>\n",
       "      <td>50.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>7.67</td>\n",
       "      <td>7.83</td>\n",
       "      <td>114.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season        GameId    GameDate GameTime            HostName  \\\n",
       "0    2013  201212090LAL  2012-12-09  6:30 pm  Los Angeles Lakers   \n",
       "1    2013  201212100PHI  2012-12-10  7:00 pm  Philadelphia 76ers   \n",
       "2    2013  201212100HOU  2012-12-10  7:00 pm     Houston Rockets   \n",
       "3    2013  201212110BRK  2012-12-11  7:00 pm       Brooklyn Nets   \n",
       "4    2013  201212110DET  2012-12-11  7:30 pm     Detroit Pistons   \n",
       "\n",
       "           GuestName  total_score  total_line  game_line  Host_HostRank  \\\n",
       "0          Utah Jazz        227.0       207.5        7.5             13   \n",
       "1    Detroit Pistons        201.0       186.5        5.5             13   \n",
       "2  San Antonio Spurs        240.0       212.0       -7.0             12   \n",
       "3    New York Knicks        197.0       195.5       -3.5             12   \n",
       "4     Denver Nuggets        195.0       203.5       -4.5             11   \n",
       "\n",
       "      ...      gPTS_avg10  gTS%_avg10  g3PAR_avg10  gFTr_avg10  gDRB%_avg10  \\\n",
       "0     ...            99.0      0.5206       0.2230      0.2981        69.22   \n",
       "1     ...            90.3      0.5077       0.2144      0.3095        71.46   \n",
       "2     ...           108.0      0.5915       0.2743      0.2518        74.26   \n",
       "3     ...           100.3      0.5473       0.3595      0.2544        74.23   \n",
       "4     ...           101.1      0.5605       0.2173      0.3177        68.45   \n",
       "\n",
       "   gTRB%_avg10  gAST%_avg10  gSTL%_avg10 gBLK%_avg10 gDRtg_avg10  \n",
       "0        50.05        61.57         8.63       10.31      110.87  \n",
       "1        49.48        59.83         6.48        9.46      107.91  \n",
       "2        50.99        61.82         8.30        6.85      101.41  \n",
       "3        47.88        52.07         9.31        7.64      109.24  \n",
       "4        50.40        56.33         7.67        7.83      114.86  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'GameId', 'GameDate', 'GameTime', 'HostName', 'GuestName',\n",
       "       'total_score', 'total_line', 'game_line', 'Host_HostRank',\n",
       "       'Host_GameRank', 'Guest_GuestRank', 'Guest_GameRank', 'host_win_count',\n",
       "       'host_lose_count', 'guest_win_count', 'guest_lose_count', 'game_behind',\n",
       "       'winner', 'loser', 'host_place_streak', 'guest_place_streak',\n",
       "       'hq1_avg10', 'hq2_avg10', 'hq3_avg10', 'hq4_avg10', 'hPace_avg10',\n",
       "       'heFG%_avg10', 'hTOV%_avg10', 'hORB%_avg10', 'hFT/FGA_avg10',\n",
       "       'hORtg_avg10', 'hFG_avg10', 'hFGA_avg10', 'hFG%_avg10', 'h3P_avg10',\n",
       "       'h3PA_avg10', 'h3P%_avg10', 'hFT_avg10', 'hFTA_avg10', 'hFT%_avg10',\n",
       "       'hORB_avg10', 'hDRB_avg10', 'hTRB_avg10', 'hAST_avg10', 'hSTL_avg10',\n",
       "       'hBLK_avg10', 'hTOV_avg10', 'hPF_avg10', 'hPTS_avg10', 'hTS%_avg10',\n",
       "       'h3PAR_avg10', 'hFTr_avg10', 'hDRB%_avg10', 'hTRB%_avg10',\n",
       "       'hAST%_avg10', 'hSTL%_avg10', 'hBLK%_avg10', 'hDRtg_avg10', 'gq1_avg10',\n",
       "       'gq2_avg10', 'gq3_avg10', 'gq4_avg10', 'gPace_avg10', 'geFG%_avg10',\n",
       "       'gTOV%_avg10', 'gORB%_avg10', 'gFT/FGA_avg10', 'gORtg_avg10',\n",
       "       'gFG_avg10', 'gFGA_avg10', 'gFG%_avg10', 'g3P_avg10', 'g3PA_avg10',\n",
       "       'g3P%_avg10', 'gFT_avg10', 'gFTA_avg10', 'gFT%_avg10', 'gORB_avg10',\n",
       "       'gDRB_avg10', 'gTRB_avg10', 'gAST_avg10', 'gSTL_avg10', 'gBLK_avg10',\n",
       "       'gTOV_avg10', 'gPF_avg10', 'gPTS_avg10', 'gTS%_avg10', 'g3PAR_avg10',\n",
       "       'gFTr_avg10', 'gDRB%_avg10', 'gTRB%_avg10', 'gAST%_avg10',\n",
       "       'gSTL%_avg10', 'gBLK%_avg10', 'gDRtg_avg10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768, 96)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    998\n",
       "2016    985\n",
       "2015    984\n",
       "2013    801\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Season.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Utah Jazz\n",
       "1    Philadelphia 76ers\n",
       "2     San Antonio Spurs\n",
       "3       New York Knicks\n",
       "4        Denver Nuggets\n",
       "Name: winner, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.winner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a column called 'host_wins' which will indicate\n",
    "#whether the host team won the game or not\n",
    "data['host_wins'] = (data['HostName'] == data['winner']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.594214\n",
       "0    0.405786\n",
       "Name: host_wins, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at the baseline accuracy for this data\n",
    "baseline = data['host_wins'].value_counts(normalize=True) \n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could definitely do a bit more EDA!  We could...\n",
    "- use .describe() to find some descriptive statistics\n",
    "- create some plots & visualisations to better understand the shapes and relationships in our data\n",
    "- use a correlation heatmap to help us find variables that could predict 'host_wins'\n",
    "\n",
    "...but we're going to skip that for this lesson!  Be aware that if you're undertaking your own data investigation, **skipping EDA is a bad idea**. (Obviously.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP TWO: Set up predictor matrix (X) and target array (y).  Dummify if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [c for c in data.columns if c not in ['GameId','GameDate','GameTime','HostName',\n",
    "                                                   'GuestName','total_score','total_line','game_line',\n",
    "                                                   'winner','loser','host_wins','Season']]\n",
    "X = data[predictors]\n",
    "y = data['host_wins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Host_HostRank           int64\n",
       "Host_GameRank           int64\n",
       "Guest_GuestRank         int64\n",
       "Guest_GameRank          int64\n",
       "host_win_count          int64\n",
       "host_lose_count         int64\n",
       "guest_win_count         int64\n",
       "guest_lose_count        int64\n",
       "game_behind           float64\n",
       "host_place_streak       int64\n",
       "guest_place_streak      int64\n",
       "hq1_avg10             float64\n",
       "hq2_avg10             float64\n",
       "hq3_avg10             float64\n",
       "hq4_avg10             float64\n",
       "hPace_avg10           float64\n",
       "heFG%_avg10           float64\n",
       "hTOV%_avg10           float64\n",
       "hORB%_avg10           float64\n",
       "hFT/FGA_avg10         float64\n",
       "hORtg_avg10           float64\n",
       "hFG_avg10             float64\n",
       "hFGA_avg10            float64\n",
       "hFG%_avg10            float64\n",
       "h3P_avg10             float64\n",
       "h3PA_avg10            float64\n",
       "h3P%_avg10            float64\n",
       "hFT_avg10             float64\n",
       "hFTA_avg10            float64\n",
       "hFT%_avg10            float64\n",
       "                       ...   \n",
       "gORB%_avg10           float64\n",
       "gFT/FGA_avg10         float64\n",
       "gORtg_avg10           float64\n",
       "gFG_avg10             float64\n",
       "gFGA_avg10            float64\n",
       "gFG%_avg10            float64\n",
       "g3P_avg10             float64\n",
       "g3PA_avg10            float64\n",
       "g3P%_avg10            float64\n",
       "gFT_avg10             float64\n",
       "gFTA_avg10            float64\n",
       "gFT%_avg10            float64\n",
       "gORB_avg10            float64\n",
       "gDRB_avg10            float64\n",
       "gTRB_avg10            float64\n",
       "gAST_avg10            float64\n",
       "gSTL_avg10            float64\n",
       "gBLK_avg10            float64\n",
       "gTOV_avg10            float64\n",
       "gPF_avg10             float64\n",
       "gPTS_avg10            float64\n",
       "gTS%_avg10            float64\n",
       "g3PAR_avg10           float64\n",
       "gFTr_avg10            float64\n",
       "gDRB%_avg10           float64\n",
       "gTRB%_avg10           float64\n",
       "gAST%_avg10           float64\n",
       "gSTL%_avg10           float64\n",
       "gBLK%_avg10           float64\n",
       "gDRtg_avg10           float64\n",
       "Length: 85, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check to make sure we don't have any categorical predictors:\n",
    "#if we don't have any categorical predictors, then we DON'T have to dummify\n",
    "#(remember, it's normally fine to have a categorical target variable)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP THREE: Train/test split and StandardScaler( ).  In this case, instead of using train_test_split, we're going to use the most recent season as our testing data (2016 data), and previous seasons as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014    998\n",
       "2016    985\n",
       "2015    984\n",
       "2013    801\n",
       "Name: Season, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your training and testing sets\n",
    "mask=(data.Season==2016)\n",
    "X_train = X[~mask]\n",
    "X_test = X[mask]\n",
    "\n",
    "y_train = y[~mask]\n",
    "y_test = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardise your predictor matrices\n",
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FOUR: Use cross-validation to optimise the hyperparameters for your model. You might try different types of models at this stage as well, and you might use GridSearchCV (or any of the other CVs like RidgeCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can fit a default `KNeighborsClassifier` to predict 'host_wins'.\n",
    "\n",
    "We can use cross-validation with our training data to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59066427, 0.57989228, 0.58527828, 0.57553957, 0.61690647])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up a default KNN model and cross-validate it on the training data\n",
    "#use 5 cross-validation folds\n",
    "knn = KNeighborsClassifier()\n",
    "cross_val_score(knn,X_train_ss,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated accuracy for default knn: 0.589656174521783\n",
      "Baseline accuracy: 1    0.594214\n",
      "0    0.405786\n",
      "Name: host_wins, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#find the mean for your cross_val_scores\n",
    "knn_cv_accuracy = cross_val_score(knn,X_train_ss,y_train,cv=5).mean()\n",
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy)\n",
    "print('Baseline accuracy:',baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default KNN performs quite poorly on the test data. But what if we **changed the number of neighbors? The weighting? The distance metric?**\n",
    "\n",
    "These are all **hyperparameters of the KNN**. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set.\n",
    "\n",
    "#### Gridsearch pseudocode for our KNN\n",
    "\n",
    "```python\n",
    "accuracies = {}\n",
    "for k in neighbors_to_test:\n",
    "    for w in weightings_to_test:\n",
    "        for d in distance_metrics_to_test:\n",
    "            hyperparam_set = (k, w, d)\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, metric=d)\n",
    "            cv_accuracies = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "            accuracies[hyperparam_set] = np.mean(cv_accuracies)\n",
    "```\n",
    "\n",
    "In the pseudocode above, we would find the key in the dictionary (a hyperparameter set) that has the larget value (mean cross-validated accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily sklearn comes with a convenience class for performing gridsearch:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the gridsearch for our KNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[1,3,5,7,9],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, cv=4, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the sklearn gridsearch below on the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [5, 9, 15, 25, 40, 50, 60], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [5,9,15,25,40,50,60],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']}\n",
    "\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), \n",
    "                              knn_params, \n",
    "                              n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "knn_gridsearch.fit(X_train_ss, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've fit our GridSearch.\n",
    "\n",
    "#### Examing the results of GridSearch( )\n",
    "\n",
    "Once the gridsearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the gridsearch object, stored as attributes:\n",
    "\n",
    "| Property | Use |\n",
    "| --- | ---|\n",
    "| **`results.param_grid`** | Displays parameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The parameters that have been found to perform with the best score. |\n",
    "| **`results.grid_scores_`** | Display score attributes with corresponding parameters. | \n",
    "\n",
    "**Print out the best score found in the search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the best mean cross-validated accuracy from the gridsearch\n",
    "#hopefully this should be much better than our default mean cross-validated accuracy \n",
    "best_knn=knn_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validated accuracy for default knn: 0.589656174521783\n",
      "Baseline accuracy: 1    0.594214\n",
      "0    0.405786\n",
      "Name: host_wins, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Mean cross-validated accuracy for default knn:',knn_cv_accuracy)\n",
    "print('Baseline accuracy:',baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the set of hyperparameters that achieved the best score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 50, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out your best hyperparameters\n",
    "knn_gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP FIVE: Once you're happy with your hyperparameters, fit your model on your whole training data and test it on your whole testing data.  (When you use a gridsearch's `.best_estimator_`, it will already have fit a model with the best hyperparameters on your training data, so all you have to do is score it on your testing data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign the best fit model (`best_estimator_`) to a variable and score it on the test data.**\n",
    "\n",
    "Compare this model to the baseline accuracy and your default KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6467005076142132"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign your best_estimator_ to the variable, then use .score( ) on your testing data\n",
    "best_knn = knn_gridsearch.best_estimator_\n",
    "best_knn.score(X_test_ss,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP SIX: Then you might want to evaluate the performance of the model (R2 score, accuracy, confusion matrix, etc); find the actual predictions that your model is providing and store them in a dataframe; plot your predictions against your actual target variable to visualise how well your model performed; investigate feature importance with .coef_ if you have a parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_home_win</th>\n",
       "      <th>predicted_home_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>predicted_home_win</th>\n",
       "      <td>501</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_home_loss</th>\n",
       "      <td>255</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     predicted_home_win  predicted_home_loss\n",
       "predicted_home_win                  501                   93\n",
       "predicted_home_loss                 255                  136"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there's lots of stuff you can do to follow up and investigate when you've found your best model,\n",
    "#but let's just look at some different ways to assess a classifier now using what you learned yesterday:\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "predictions = best_knn.predict(X_test_ss)\n",
    "confusion = confusion_matrix(y_test,predictions,labels=[1,0])\n",
    "pd.DataFrame(confusion, \n",
    "             columns=['predicted_home_win','predicted_home_loss'], \n",
    "             index=['predicted_home_win','predicted_home_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434343434343434"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall is concerned with how much you catch all the positives:\n",
    "#it's more important with cancer detection tests, for example: \n",
    "#     \"Let's make sure we RECALL all the patients with positives to do further testing!\" \n",
    "#     \"Yes!  But let's make sure we're SENSITIVE about how we tell them they might have cancer.\"\n",
    "#it's the true positives, divided by all the actual positives\n",
    "(501/(501+93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434343434343434"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check that:\n",
    "recall_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626984126984127"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision is concerned with how precisely you predict positives; in other words,\n",
    "#when you predict a positive, are you pretty sure about that prediction?\n",
    "#it's more important with a spam filter, for example:\n",
    "#    \"Whhaaattttt. An email from the Home Office went to my spam filter?!\n",
    "#    \"Spam filters are great, but they damn sure better be PRECISE about what emails they think are spam!\"\n",
    "#it's the true positives, divided by all the predicted positives\n",
    "(501/(501+255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626984126984127"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check that:\n",
    "precision_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7422222222222222, 0.7422222222222222)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the F1 score is the harmonic mean between these two metrics:\n",
    "2*(501/(501+255))*(501/(501+93))/((501/(501+255))+(501/(501+93))),f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding checkout (in pair - 8min):** How can you modify the recall or the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016835016835016834"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A:\n",
    "mod=best_knn.predict_proba(X_test_ss)\n",
    "new_mod=[int(i[1]>0.9) for i in mod]\n",
    "recall_score(y_test,new_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "## Independent practice: gridsearch regularization penalties with logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression models can also apply the Lasso and Ridge penalties. The `LogisticRegression` class takes these regularization-relevant hyperparameters:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`penalty`** | `'l1'` for Lasso, `'l2'` for Ridge |\n",
    "| **`solver`** | Must be set to `'liblinear'` for the Lasso penalty to work. |\n",
    "| **`C`** | The regularization strength. Equivalent to `1./alpha` |\n",
    "\n",
    "**You should:**\n",
    "1. Fit and validate the accuracy of a default logistic regression on the basketball data.\n",
    "- Perform a gridsearch over different regularization strengths and Lasso and Ridge penalties.\n",
    "- Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy and the default model.\n",
    "- Look at the best parameters found. What was chosen? What does this suggest about our data?\n",
    "- Look at the coefficients and associated predictors for your optimized model. What appears to be the most important predictors of winning the game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a logistic regression model, \n",
    "#and find its cross-validated scores for your training data\n",
    "#you should get accuracies higher than 0.6, which is better than KNN!\n",
    "lr = \n",
    "cross_val_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean of those cross-validated scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters. \n",
    "# Use a list with 'l1' and 'l2' for the penalties,\n",
    "# Use a list with 'liblinear' for the solver,\n",
    "# Use a logspace from -3 to 0, with 50 different values\n",
    "\n",
    "# fill the dictionary of parameters\n",
    "gs_params = {'penalty':?,\n",
    "             'solver':?,\n",
    "             'C':?}\n",
    "\n",
    "#create your gridsearch object\n",
    "lr_gridsearch = GridSearchCV()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit your gridsearch object on your training data\n",
    "lr_gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best mean cross-validated score that your gridsearch found:\n",
    "# (this should be better than the mean cross-validated score for your default logistic regression above)\n",
    "lr_gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best hyperparameters that your gridsearch found:\n",
    "lr_gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best estimator to a variable:\n",
    "best_lr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score your best estimator on the testing data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's analyse the features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to look at the coefficients\n",
    "coef_df = pd.DataFrame({'coef': best_lr.coef_[0],\n",
    "                        'feature': X.columns,\n",
    "                        'abs_coef': np.abs(best_lr.coef_[0])})\n",
    "\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEY TAKEAWAYS:\n",
    "\n",
    "- You always want to use your training data to search for your best hyperparameters! You can do this with GridSearchCV, or with other sklearn objects like RidgeCV, LassoCV, ElasticNetCV, or LogisticRegressionCV.  \n",
    "\n",
    "\n",
    "- You instantiate GridSearchCV with:\n",
    "    - a model\n",
    "    - a dictionary for that model's parameters\n",
    "    - the number of cross-validation folds you want it to perform (`cv=`)\n",
    "    - how many cores to use on your computer for this job (`n_jobs=`)\n",
    "    - whether you want your model to give you some print-outs as it works (`verbose=`)\n",
    "    \n",
    "\n",
    "- Once you've instantiated the GridSearch object, you can fit it on your training data\n",
    "\n",
    "\n",
    "- Once it's finished searching, you can access some useful attributes:\n",
    "    - `.best_score_`, to find the mean cross-validated score of the best estimator\n",
    "    - `.best_params_`, to find the best hyperparameters \n",
    "    - `.best_estimator_`, which you will assign to a variable in order to use `.score()`, `.predict()`, `.coef_`, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRESS CHECKPOINT:\n",
    "\n",
    "- EVERYONE should be able to:\n",
    "    - give an example of a hyperparameter\n",
    "    - explain the basic intuition behind how GridSearch works\n",
    "    - copy and paste the GridSearching from this lesson and adapt it to a different lab to find the best hyperparameters for a model\n",
    "\n",
    "\n",
    "\n",
    "- MANY of you should be able to:\n",
    "    -  use your best estimator from the gridsearch to evaluate model effectiveness and investigate feature importance\n",
    "    -  come up with reasonable parameter dictionaries by yourself for KNN and Logistic Regression\n",
    "    -  create a confusion matrix using your predictions from your best estimator, and understand how to interpret it\n",
    "\n",
    "\n",
    "- SOME of you should be able to:\n",
    "    - look at the documentation for GridSearchCV on sklearn's website, and investigate the different options that are available (for example, the attribute `.cv_results_`)\n",
    "    - create a ROC curve and a Precision-Recall curve for your predictions, and be able to interpret them\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
