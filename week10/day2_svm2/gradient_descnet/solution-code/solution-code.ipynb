{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Code-Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through how gradient descent works using code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.random.uniform(-10, 80, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.7086107 , 75.56428758, 55.87945476, 43.87926358,  4.04167764,\n",
       "        4.03950683, -4.7724749 , 67.95585312, 44.10035106, 53.726532  ,\n",
       "       -8.14739551, 77.29188669, 64.91983767,  9.11051996,  6.36424705,\n",
       "        6.50640589, 17.38180187, 37.22807885, 28.87505168, 16.21062262,\n",
       "       45.06676053,  2.55444746, 16.29301837, 22.9725659 , 31.04629858,\n",
       "       60.66583653,  7.97064039, 36.28109946, 43.3173112 , -5.81946286,\n",
       "       44.67903667,  5.34717113, -4.14535663, 75.39969835, 76.90688298,\n",
       "       62.75576133, 17.41523923, -1.20950974, 51.58097239, 29.61372444,\n",
       "        0.98344114, 34.56592191, -6.9050331 , 71.83883619, 13.29019834,\n",
       "       49.62700559, 18.05399685, 36.80612191, 39.20392514,  6.636901  ,\n",
       "       77.2626165 , 59.7619541 , 74.55490474, 70.53446154, 43.81099809,\n",
       "       72.96868115, -2.03567482,  7.63845762, -5.929544  , 19.27972977,\n",
       "       24.98095607, 14.42141286, 64.58637582, 22.1077994 , 15.28410587,\n",
       "       38.84264748,  2.68318025, 62.19772827, -3.29044207, 78.81982429,\n",
       "       59.50202924,  7.88441134, -9.50300946, 63.39152856, 53.61716095,\n",
       "       55.61064512, 59.4143312 , -3.33598134, 22.26191557,  0.42821536,\n",
       "       67.67930833, 46.09683141, 19.78082224, -4.27974847, 17.98840895,\n",
       "       19.26649898, 55.66455605, 47.38017242, 69.84914683, 32.49934326,\n",
       "        0.76348213, 54.19203085, 58.47065438, 40.51494778, 59.3870462 ,\n",
       "       34.44160367, 37.04595464, 28.47869165, -7.71227859, -0.28977157])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.31626690403884\n",
      "26.77404699137873\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(temp))\n",
    "print(np.std(temp, ddof = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ohio State Fun Facts:**\n",
    "1. Ohio Stadium can seat 104,944 people. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Ohio_Stadium).)\n",
    "2. Ohio Stadium's record attendance is 110,045 people. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Ohio_Stadium).)\n",
    "3. Michigan sucks. (Source: It's just a fact.)\n",
    "4. Ohio State students enjoy alcohol. (Source: first-hand knowledge.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_sold = 200000 + 1000 * temp + np.random.normal(0, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225449.55206103, 277305.22894166, 257620.39612779, 245620.2049425 ,\n",
       "       205782.61900458, 205780.44819502, 196968.4664599 , 269696.79448451,\n",
       "       245841.29242165, 255467.47336641, 193593.54585139, 279032.82805934,\n",
       "       266660.7790368 , 210851.46132581, 208105.1884134 , 208247.34725157,\n",
       "       219122.74323112, 238969.02021166, 230615.99304255, 217951.56398259,\n",
       "       246807.70188978, 204295.38882345, 218033.95973293, 224713.5072612 ,\n",
       "       232787.2399443 , 262406.77789013, 209711.58175902, 238022.04082199,\n",
       "       245058.25256235, 195921.47850956, 246419.97803589, 207088.11249662,\n",
       "       197595.58473344, 277140.63971756, 278647.82434147, 264496.70269524,\n",
       "       219156.18059037, 200531.43162534, 253321.91375086, 231354.66580133,\n",
       "       202724.38250079, 236306.86327478, 194835.90826513, 273579.77755185,\n",
       "       215031.13970876, 251367.94695662, 219794.93821281, 238547.06327077,\n",
       "       240944.86650566, 208377.84236206, 279003.55786357, 261502.89546726,\n",
       "       276295.84610554, 272275.40290325, 245551.93945776, 274709.62251684,\n",
       "       199705.26654944, 209379.39898249, 195811.39736671, 221020.67113346,\n",
       "       226721.89743682, 216162.35422441, 266327.31718844, 223848.74076719,\n",
       "       217025.04723663, 240583.58884901, 204424.12161249, 263938.66963263,\n",
       "       198450.49929594, 280560.76565881, 261242.97060146, 209625.35270284,\n",
       "       192237.93190589, 265132.4699257 , 255358.10231105, 257351.58648845,\n",
       "       261155.2725665 , 198404.96002083, 224002.85693375, 202169.15672203,\n",
       "       269420.24969357, 247837.77277924, 221521.7636015 , 197461.19289051,\n",
       "       219729.35031917, 221007.44034717, 257405.49741519, 249121.11378673,\n",
       "       271590.08819663, 234240.28462934, 202504.42349921, 255932.97221483,\n",
       "       260211.59574028, 242255.88914602, 261127.98756067, 236182.54503756,\n",
       "       238786.89600914, 230219.63301703, 194028.66277173, 201451.16979416])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beers_sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{beers_sold}_i = 200000 + 1000 * \\text{temp}_i + \\varepsilon_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'temp': temp,\n",
    "                             'beers_sold': beers_sold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beers_sold</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225449.552061</td>\n",
       "      <td>23.708611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277305.228942</td>\n",
       "      <td>75.564288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257620.396128</td>\n",
       "      <td>55.879455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245620.204942</td>\n",
       "      <td>43.879264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205782.619005</td>\n",
       "      <td>4.041678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beers_sold       temp\n",
       "0  225449.552061  23.708611\n",
       "1  277305.228942  75.564288\n",
       "2  257620.396128  55.879455\n",
       "3  245620.204942  43.879264\n",
       "4  205782.619005   4.041678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to fit a model here.\n",
    "- You and I know that our $y$-intercept $\\beta_0$ is 200,000.\n",
    "- You and I know that our slope $\\beta_1$ is 1,000.\n",
    "- However, our computer does not know that. Our computer has to estimate $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ from the data.\n",
    "    - We might say that our **machine** has to... **learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our workflow:\n",
    "1. Instantiate model.\n",
    "2. Select a learning rate $\\alpha$.\n",
    "3. Select a starting point $\\hat{\\beta}_{1,0}$.\n",
    "4. Calculate the gradient of the loss function.\n",
    "5. Calculate $\\hat{\\beta}_{1,i+1} = \\hat{\\beta}_{1,i} - \\alpha * \\frac{\\partial L}{\\partial \\beta_1}$.\n",
    "6. Check value of $|\\hat{\\beta}_{1,i+1} - \\hat{\\beta}_{1,i}|$.\n",
    "7. Repeat steps 4 through 6 until \"stopping condition\" is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Instantiate model.\n",
    "\n",
    "Our model takes on the form:\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Select a learning rate $\\alpha$.\n",
    "\n",
    "$$\\alpha = 0.1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Select a starting point.\n",
    "The zero-th iteration of $\\hat{\\beta}_1$ is going to start at, say, 20.\n",
    "$$\\hat{\\beta}_{1,0} = 20$$\n",
    "\n",
    "Two points:\n",
    "- You and I know that the true value of $\\beta_1$ is 1000. We need the computer to figure that part out!\n",
    "- We're going to pretend like the computer already knows the value for $\\beta_0$. In reality, we'd have to do this for $\\beta_0$ and for $\\beta_1$ at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_1 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Calculate the gradient of the loss function with respect to parameter $\\beta_1$.\n",
    "\n",
    "The loss function, $L$, is our mean square error.\n",
    "\n",
    "$$L = \\sum_{i = 1} ^ n (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$$\\Rightarrow L = \\sum_{i = 1} ^ n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2 $$\n",
    "\n",
    "The gradient of this loss function with respect to $\\beta_1$ is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta_1} = \\frac{2}{n} \\sum_{i=1}^n -x_i(y_i - (\\hat{\\beta}_1x_i + \\hat{\\beta}_0)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beta_1_gradient(x, y, beta_1, beta_0):\n",
    "    n = len(x)\n",
    "    gradient = 0\n",
    "    for i in range(n):\n",
    "        gradient += -1 * x[i] * (y[i] - (beta_1 * x[i] + beta_0))\n",
    "    gradient *= (2 / n)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Calculate $\\hat{\\beta}_{1,i+1} = \\hat{\\beta}_{1,i} - \\alpha * \\frac{\\partial L}{\\partial \\beta_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_beta_1(beta_1, alpha, gradient):\n",
    "    beta_1 = beta_1 - alpha * gradient\n",
    "    return beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Check value of $|\\hat{\\beta}_{1,i+1} - \\hat{\\beta}_{1,i}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_update(beta_1, updated_beta_1, tolerance = 0.1):\n",
    "    if abs(beta_1 - updated_beta_1) < tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Save final value of $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta_1 = 0, alpha = 0.01, max_iter = 100):\n",
    "    converged = False\n",
    "    for i in range(max_iter):\n",
    "        gradient = beta_1_gradient(x, y, beta_1, 200000)\n",
    "        updated_beta_1 = update_beta_1(beta_1, alpha, gradient)\n",
    "        converged = check_update(beta_1, updated_beta_1)\n",
    "        beta_1 = updated_beta_1\n",
    "        if converged == True:\n",
    "            print(\"Our algorithm converged after \" + str(i) + \"iterations with a beta_1 value of: \"+ str(beta_1))\n",
    "            break\n",
    "        print(\"Iteration \" + str(i) + \" with beta_1 value of: \" + str(beta_1))\n",
    "    if converged == False:\n",
    "        print(\"Our algorithm did not converge, so do not trust the value of beta_1.\")\n",
    "    return beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with beta_1 value of: 375.04049694698205\n",
      "Iteration 1 with beta_1 value of: 605.5312109730789\n",
      "Iteration 2 with beta_1 value of: 755.1647590023708\n",
      "Iteration 3 with beta_1 value of: 852.3061939206527\n",
      "Iteration 4 with beta_1 value of: 915.3699821036045\n",
      "Iteration 5 with beta_1 value of: 956.3107133510871\n",
      "Iteration 6 with beta_1 value of: 982.8892542756507\n",
      "Iteration 7 with beta_1 value of: 1000.1439250192795\n",
      "Iteration 8 with beta_1 value of: 1011.3455806449984\n",
      "Iteration 9 with beta_1 value of: 1018.6176457351731\n",
      "Iteration 10 with beta_1 value of: 1023.3386380994017\n",
      "Iteration 11 with beta_1 value of: 1026.4034853782327\n",
      "Iteration 12 with beta_1 value of: 1028.3931706218973\n",
      "Iteration 13 with beta_1 value of: 1029.6848654466933\n",
      "Iteration 14 with beta_1 value of: 1030.5234279910542\n",
      "Iteration 15 with beta_1 value of: 1031.0678190711346\n",
      "Iteration 16 with beta_1 value of: 1031.4212353429014\n",
      "Iteration 17 with beta_1 value of: 1031.650671617153\n",
      "Iteration 18 with beta_1 value of: 1031.799620627563\n",
      "Our algorithm converged after 19iterations with a beta_1 value of: 1031.8963176636985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1031.8963176636985"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(df['temp'], df['beers_sold'], beta_1 = 20, alpha = 0.0001, max_iter = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
